{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63c2f026",
   "metadata": {},
   "source": [
    "# üåßÔ∏è Green & Ampt Infiltration Processor\n",
    "\n",
    "The FLO-2D model includes the Green-Ampt infiltration method as one of its core infiltration engines. This approach is used in hydrologic modeling because it effectively captures transmission losses as water infiltrates into the soil.\n",
    "\n",
    "In the FLO-2D model, rainfall is distributed to the computational grid, where it infiltrates into the soil until reaching the saturation depth or fill volume. This continuous process accounts for the dynamic movement of water through the watershed as flood routing progresses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1c05039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Base path automatically set to: c:\\Users\\Karen\\VS Code Projects\\ASFPM-LLM-Data-Management-Workshop\n"
     ]
    }
   ],
   "source": [
    "# Automatically set base path to the project directory where the notebook is running\n",
    "from pathlib import Path\n",
    "\n",
    "# This gets the directory where the current notebook is located\n",
    "base_path = Path.cwd()\n",
    "\n",
    "print(f\"üìÇ Base path automatically set to: {base_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57286347",
   "metadata": {},
   "source": [
    "# üåßÔ∏è Find the fields "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb4314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Print the fields of vector files in the project directory. This script prints the \n",
    "fields of shapefiles and GeoPackage files in the specified directory.  The resulting \n",
    "list can be used to fill the fields list in the next processor which will write a \n",
    "raster file for each field.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from osgeo import ogr, gdal\n",
    "import shapefile  # pyshp\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Suppress GDAL exception warnings\n",
    "ogr.UseExceptions()\n",
    "gdal.UseExceptions()\n",
    "\n",
    "\n",
    "# File paths\n",
    "soil_shapefile = base_path / 'Data' / 'Infiltration' / 'Soil 2023.shp'\n",
    "landuse_shapefile = base_path / 'Data' / 'Infiltration' / 'Land Use 2018.shp'\n",
    "grid_layer_path = base_path / 'Data' / 'GeoPackage' / 'selfhelp.gpkg'\n",
    "\n",
    "def print_fields(filepath):\n",
    "    if filepath.suffix == '.shp':\n",
    "        # Use pyshp for shapefiles\n",
    "        sf = shapefile.Reader(str(filepath))\n",
    "        fields = [f[0] for f in sf.fields[1:]]  # Skip deletion flag\n",
    "        sf.close()\n",
    "    elif filepath.suffix == '.gpkg':\n",
    "        # Use ogr for GeoPackage\n",
    "        ds = ogr.Open(str(filepath))\n",
    "        layer = ds.GetLayer()\n",
    "        fields = [field.name for field in layer.schema]\n",
    "        ds = None\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Unsupported file type: {filepath}\")\n",
    "        return\n",
    "\n",
    "    # Print the fields\n",
    "    print(f\"\\nFields in {filepath.name}:\")\n",
    "    for field in fields:\n",
    "        print(f\" - {field}\")\n",
    "\n",
    "# Print the fields for each vector file\n",
    "print_fields(soil_shapefile)\n",
    "print_fields(landuse_shapefile)\n",
    "print_fields(grid_layer_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2664fd3",
   "metadata": {},
   "source": [
    "# üåßÔ∏è Rasterize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1837c445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from osgeo import gdal, ogr\n",
    "import shapefile  # pyshp\n",
    "import tempfile\n",
    "\n",
    "# Define input paths\n",
    "soil_shapefile = base_path / 'Data' / 'Infiltration' / 'Soil 2023.shp'\n",
    "landuse_shapefile = base_path / 'Data' / 'Infiltration' / 'Land Use 2018.shp'\n",
    "grid_layer_path = base_path / 'Data' / 'GeoPackage' / 'selfhelp.gpkg'\n",
    "output_folder = base_path / 'Data' / 'Infiltration' / 'Rasters'\n",
    "\n",
    "# Fields to rasterize.  Make sure these fields are in the results printed by the previous cell.\n",
    "soil_fields = ['hydc', 'soil_depth', 'psif', 'dthetad', 'dthetan', 'dthetaw']\n",
    "landuse_fields = ['IA', 'RTIMP', 'VC', 'InitSat']\n",
    "\n",
    "# EPSG code for NAD83 Central AZ\n",
    "target_epsg = 2223\n",
    "\n",
    "import shapefile\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "def preprocess_sat_field(shapefile_path, field_name=\"InitSat\"):\n",
    "    \"\"\"\n",
    "    Preprocess the InitSat field in the shapefile to convert categorical values to numeric.  \n",
    "    Converts:\n",
    "    - \"wet\" to 0\n",
    "    - \"normal\" to 1\n",
    "    - \"dry\" to 2\n",
    "    - Anything else to -1 (unknown)\n",
    "    \"\"\"\n",
    "    # Load the shapefile\n",
    "    reader = shapefile.Reader(shapefile_path)\n",
    "    fields = reader.fields[1:]  # skip DeletionFlag\n",
    "    field_names = [f[0] for f in fields]\n",
    "\n",
    "    if field_name not in field_names:\n",
    "        raise ValueError(f\"Field '{field_name}' not found in {shapefile_path}\")\n",
    "\n",
    "    # Find the index of the InitSat field\n",
    "    sat_index = field_names.index(field_name)\n",
    "\n",
    "    # Create a temporary directory for the processed shapefile\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    print(f\"Temp directory for Sat processing: {temp_dir}\")\n",
    "    temp_shp = os.path.join(temp_dir, \"processed.shp\")\n",
    "\n",
    "    # Create the new shapefile with the updated InitSat field\n",
    "    writer = shapefile.Writer(temp_shp)\n",
    "    for field in fields:\n",
    "        if field[0] == field_name:\n",
    "            writer.field(field_name, 'N', decimal=0)\n",
    "        else:\n",
    "            writer.field(*field)\n",
    "\n",
    "    # Process each record\n",
    "    record_count = 0\n",
    "    for sr in reader.shapeRecords():\n",
    "        rec = list(sr.record)\n",
    "        val = str(rec[sat_index]).strip()\n",
    "        if val == \"wet\":\n",
    "            rec[sat_index] = 0\n",
    "        elif val == \"normal\":\n",
    "            rec[sat_index] = 1\n",
    "        elif val == \"dry\":\n",
    "            rec[sat_index] = 2\n",
    "        else:\n",
    "            rec[sat_index] = -1  # unknown category\n",
    "\n",
    "        writer.shape(sr.shape)\n",
    "        writer.record(*rec)\n",
    "        record_count += 1\n",
    "\n",
    "    writer.close()\n",
    "    print(f\"Processed {record_count} records to {temp_shp}\")\n",
    "    return temp_shp\n",
    "\n",
    "\n",
    "# Rasterization properties calculated from the grid layer.\n",
    "def rasterize_field(input_vector, field, reference_layer, output_path):\n",
    "    \"\"\"\"\n",
    "    Rasterizes a field from a vector file to a raster file using the specified reference layer.\n",
    "    \"\"\"\n",
    "\n",
    "    grid_ds = gdal.OpenEx(reference_layer, gdal.OF_VECTOR)\n",
    "    grid_layer = grid_ds.GetLayer()\n",
    "    xmin, xmax, ymin, ymax = grid_layer.GetExtent()\n",
    "    xres = 30\n",
    "    yres = 30\n",
    "    cols = int((xmax - xmin) / xres)\n",
    "    rows = int((ymax - ymin) / yres)\n",
    "\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    out_raster = driver.Create(output_path, cols, rows, 1, gdal.GDT_Float32)\n",
    "    out_raster.SetGeoTransform((xmin, xres, 0, ymax, 0, -yres))\n",
    "\n",
    "    srs = ogr.osr.SpatialReference()\n",
    "    srs.ImportFromEPSG(target_epsg)\n",
    "    out_raster.SetProjection(srs.ExportToWkt())\n",
    "\n",
    "    vector_ds = ogr.Open(input_vector)\n",
    "    vector_layer = vector_ds.GetLayer()\n",
    "\n",
    "    gdal.RasterizeLayer(out_raster, [1], vector_layer, options=[\n",
    "        f\"ATTRIBUTE={field}\",\n",
    "        \"ALL_TOUCHED=TRUE\"\n",
    "    ])\n",
    "    print(f\"Raster saved: {output_path}\")\n",
    "\n",
    "# Make sure output directory exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Rasterize Soil fields\n",
    "for field in soil_fields:\n",
    "    out_path = os.path.join(output_folder, f\"{field}.tif\")\n",
    "    rasterize_field(soil_shapefile, field, grid_layer_path, out_path)\n",
    "\n",
    "# Rasterize Land Use fields\n",
    "for field in landuse_fields:\n",
    "    input_path = landuse_shapefile\n",
    "\n",
    "    # Preprocess InitSat specifically\n",
    "    if field == \"InitSat\":\n",
    "        print(f\"Preprocessing {field} for numeric conversion...\")\n",
    "        # Corrected shapefile path\n",
    "        input_path = preprocess_sat_field(str(landuse_shapefile), field)\n",
    "\n",
    "    out_path = os.path.join(output_folder, f\"{field}.tif\")\n",
    "    print(f\"Rasterizing {field} from {input_path} to {out_path}\")\n",
    "    rasterize_field(input_path, field, grid_layer_path, out_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c60b9f",
   "metadata": {},
   "source": [
    "# üåßÔ∏è Process GeoPackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6393734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Could not find a field named 'fid'. Using feature ID instead.\n",
      "Extracted 54306 centroids from grid layer\n",
      "Found 10 raster files to process\n",
      "Processing dthetad...\n",
      "  Sampled 54306 valid values out of 54306 points\n",
      "Processing dthetan...\n",
      "  Sampled 54306 valid values out of 54306 points\n",
      "Processing dthetaw...\n",
      "  Sampled 54306 valid values out of 54306 points\n",
      "Processing hydc...\n",
      "  Sampled 54306 valid values out of 54306 points\n",
      "Processing IA...\n",
      "  Sampled 54306 valid values out of 54306 points\n",
      "Processing InitSat...\n",
      "  Sampled 54306 valid values out of 54306 points\n",
      "Processing psif...\n",
      "  Sampled 54306 valid values out of 54306 points\n",
      "Processing RTIMP...\n",
      "  Sampled 54306 valid values out of 54306 points\n",
      "Processing soil_depth...\n",
      "  Sampled 54306 valid values out of 54306 points\n",
      "Processing VC...\n",
      "  Sampled 54306 valid values out of 54306 points\n",
      "\n",
      "Processing complete!\n",
      "Created 'green_ampt_sampled' table with 54306 rows and 11 columns\n",
      "Columns: fid, dthetad, dthetan, dthetaw, hydc, IA, InitSat, psif, RTIMP, soil_depth, VC\n",
      "Warning: Could not disable WAL mode: database is locked\n",
      "Database connection closed and cleanup attempted\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Grid Centroid Raster Sampling Script\n",
    "-----------------------------------\n",
    "\n",
    "This script extracts centroids from a grid polygon layer in a GeoPackage and samples \n",
    "values from multiple rasters at those centroid locations. The results are stored in \n",
    "a new table within the GeoPackage.\n",
    "\n",
    "Functionality:\n",
    "1. Extracts centroids from polygons in the 'grid' layer\n",
    "2. Handles coordinate system transformations between grid and rasters\n",
    "3. Samples each raster at each centroid location\n",
    "4. Creates a new table with FID and sampled values from each raster\n",
    "\n",
    "Requirements:\n",
    "- GDAL/OGR\n",
    "- Pandas\n",
    "- NumPy\n",
    "- SQLite3\n",
    "- Pathlib\n",
    "\n",
    "Usage:\n",
    "1. Set the base_path variable to point to your project directory\n",
    "2. Ensure your GeoPackage has a 'grid' polygon layer\n",
    "3. Place your raster files in the specified rasters directory\n",
    "4. Run the script to create a 'green_ampt_sampled' table in your GeoPackage\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from osgeo import gdal, ogr, osr\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Define base path - update this to your project directory if necessary\n",
    "# base_path = Path(r\"C:\\Users\\Karen\\VS Code Projects\\ASFPM-LLM-Data-Management-Workshop\")\n",
    "\n",
    "# Define paths using pathlib\n",
    "gpkg_path = base_path / 'Data' / 'GeoPackage' / 'selfhelp.gpkg'\n",
    "rasters_dir = base_path / 'Data' / 'Infiltration' / 'Rasters'\n",
    "\n",
    "# Connect to the geopackage\n",
    "conn = sqlite3.connect(gpkg_path)\n",
    "\n",
    "# Use GDAL to access the grid layer and extract centroids\n",
    "driver = ogr.GetDriverByName('GPKG')\n",
    "data_source = driver.Open(str(gpkg_path), 0)  # 0 = read-only\n",
    "\n",
    "if not data_source:\n",
    "    raise Exception(f\"Could not open {gpkg_path}\")\n",
    "\n",
    "layer = data_source.GetLayerByName('grid')\n",
    "if not layer:\n",
    "    raise Exception(\"Could not access 'grid' layer\")\n",
    "\n",
    "# Get the spatial reference of the grid layer\n",
    "grid_srs = layer.GetSpatialRef()\n",
    "\n",
    "# Get the layer definition to check available fields\n",
    "layer_defn = layer.GetLayerDefn()\n",
    "field_count = layer_defn.GetFieldCount()\n",
    "field_names = []\n",
    "for i in range(field_count):\n",
    "    field_names.append(layer_defn.GetFieldDefn(i).GetName())\n",
    "\n",
    "# Try to find the FID field (case-insensitive search)\n",
    "fid_field = None\n",
    "for field in field_names:\n",
    "    if field.lower() == 'fid':\n",
    "        fid_field = field\n",
    "        break\n",
    "\n",
    "if not fid_field:\n",
    "    print(\"WARNING: Could not find a field named 'fid'. Using feature ID instead.\")\n",
    "\n",
    "# Extract centroids from grid polygons\n",
    "fid_list = []\n",
    "x_list = []\n",
    "y_list = []\n",
    "\n",
    "for feature in layer:\n",
    "    # Use the correct FID field if found, otherwise use feature ID\n",
    "    if fid_field:\n",
    "        fid = feature.GetField(fid_field)\n",
    "    else:\n",
    "        fid = feature.GetFID()  # Use feature ID as fallback\n",
    "    \n",
    "    geom = feature.GetGeometryRef()\n",
    "    if geom:\n",
    "        centroid = geom.Centroid()\n",
    "        fid_list.append(fid)\n",
    "        x_list.append(centroid.GetX())\n",
    "        y_list.append(centroid.GetY())\n",
    "\n",
    "# Create dataframe with centroids\n",
    "centroids_df = pd.DataFrame({\n",
    "    'fid': fid_list,\n",
    "    'centroid_x': x_list,\n",
    "    'centroid_y': y_list\n",
    "})\n",
    "print(f\"Extracted {len(centroids_df)} centroids from grid layer\")\n",
    "\n",
    "# Release the data source\n",
    "data_source = None\n",
    "\n",
    "# Define coordinate transformation function\n",
    "def transform_point(x, y, source_srs, target_srs):\n",
    "    \"\"\"Transform coordinates from source to target spatial reference system\"\"\"\n",
    "    if source_srs.IsSame(target_srs):\n",
    "        return x, y\n",
    "    \n",
    "    transform = osr.CoordinateTransformation(source_srs, target_srs)\n",
    "    point = ogr.CreateGeometryFromWkt(f\"POINT ({x} {y})\")\n",
    "    point.Transform(transform)\n",
    "    return point.GetX(), point.GetY()\n",
    "\n",
    "# List all raster files\n",
    "raster_files = list(rasters_dir.glob('*.tif')) + list(rasters_dir.glob('*.tiff')) + \\\n",
    "               list(rasters_dir.glob('*.img')) + list(rasters_dir.glob('*.asc'))\n",
    "\n",
    "print(f\"Found {len(raster_files)} raster files to process\")\n",
    "\n",
    "# Create a dictionary to store sampled values\n",
    "sampled_values = {'fid': centroids_df['fid'].tolist()}\n",
    "\n",
    "# Sample each raster at the centroid locations\n",
    "for raster_file in raster_files:\n",
    "    raster_name = raster_file.stem\n",
    "    print(f\"Processing {raster_name}...\")\n",
    "    \n",
    "    # Open the raster\n",
    "    raster = gdal.Open(str(raster_file))\n",
    "    if raster is None:\n",
    "        print(f\"  Could not open {raster_file}, skipping\")\n",
    "        continue\n",
    "    \n",
    "    # Get raster spatial reference\n",
    "    raster_srs = osr.SpatialReference()\n",
    "    raster_srs.ImportFromWkt(raster.GetProjection())\n",
    "    \n",
    "    # Get raster geotransform and band\n",
    "    gt = raster.GetGeoTransform()\n",
    "    rb = raster.GetRasterBand(1)\n",
    "    no_data_value = rb.GetNoDataValue()\n",
    "    \n",
    "    # Initialize list for this raster's values\n",
    "    values = []\n",
    "    \n",
    "    # For each centroid, sample the raster\n",
    "    for idx, row in centroids_df.iterrows():\n",
    "        try:\n",
    "            x, y = row['centroid_x'], row['centroid_y']\n",
    "            \n",
    "            # Skip if coordinates are None or NaN\n",
    "            if x is None or y is None or pd.isna(x) or pd.isna(y):\n",
    "                values.append(None)\n",
    "                continue\n",
    "            \n",
    "            # Transform coordinates from grid CRS to raster CRS if needed\n",
    "            if grid_srs and raster_srs and not grid_srs.IsSame(raster_srs):\n",
    "                try:\n",
    "                    x, y = transform_point(x, y, grid_srs, raster_srs)\n",
    "                except Exception:\n",
    "                    values.append(None)\n",
    "                    continue\n",
    "            \n",
    "            # Convert from map coordinates to pixel coordinates\n",
    "            px = int((x - gt[0]) / gt[1])\n",
    "            py = int((y - gt[3]) / gt[5])\n",
    "            \n",
    "            # Check if pixel is within raster bounds\n",
    "            if px >= 0 and px < raster.RasterXSize and py >= 0 and py < raster.RasterYSize:\n",
    "                # Read the pixel value\n",
    "                data = rb.ReadAsArray(px, py, 1, 1)\n",
    "                if data is not None:\n",
    "                    value = data[0][0]\n",
    "                    # Check if the value is NoData\n",
    "                    if no_data_value is not None and value == no_data_value:\n",
    "                        values.append(None)\n",
    "                    else:\n",
    "                        values.append(float(value))\n",
    "                else:\n",
    "                    values.append(None)\n",
    "            else:\n",
    "                values.append(None)\n",
    "        except Exception:\n",
    "            values.append(None)\n",
    "    \n",
    "    # Add values to the dictionary\n",
    "    sampled_values[raster_name] = values\n",
    "    \n",
    "    # Calculate some basic statistics\n",
    "    non_null_values = [v for v in values if v is not None]\n",
    "    print(f\"  Sampled {len(non_null_values)} valid values out of {len(values)} points\")\n",
    "    \n",
    "    # Close the raster\n",
    "    raster = None\n",
    "\n",
    "# Create a dataframe from the sampled values\n",
    "result_df = pd.DataFrame(sampled_values)\n",
    "\n",
    "# Replace NaN values with NULL for SQLite compatibility\n",
    "result_df = result_df.replace({np.nan: None})\n",
    "\n",
    "# Create the new table in the geopackage\n",
    "result_df.to_sql('green_ampt_sampled', conn, if_exists='replace', index=False)\n",
    "\n",
    "print(\"\\nProcessing complete!\")\n",
    "print(f\"Created 'green_ampt_sampled' table with {len(result_df)} rows and {len(result_df.columns)} columns\")\n",
    "print(\"Columns: \" + \", \".join(result_df.columns))\n",
    "\n",
    "# Close the connection with proper cleanup\n",
    "def close_connection_with_cleanup(connection):\n",
    "    # First, we disable WAL mode (Write-Ahead Logging) to help release the -wal and -shm files\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(\"PRAGMA journal_mode=DELETE;\")\n",
    "        cursor.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not disable WAL mode: {e}\")\n",
    "    \n",
    "    # Commit any pending transactions\n",
    "    try:\n",
    "        connection.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not commit: {e}\")\n",
    "    \n",
    "    # Close the connection\n",
    "    try:\n",
    "        connection.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error closing connection: {e}\")\n",
    "    \n",
    "    print(\"Database connection closed and cleanup attempted\")\n",
    "\n",
    "# Use the function to close the connection\n",
    "close_connection_with_cleanup(conn)\n",
    "\n",
    "# If you're using GDAL/OGR data sources, make sure those are closed too\n",
    "# For example, if you have a data_source variable:\n",
    "# data_source = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5fcb2a",
   "metadata": {},
   "source": [
    "\n",
    "# ü§ñ Instructions for Students: Next Steps with VS Code Copilot\n",
    "\n",
    "## Overview\n",
    "You've successfully created a script that extracts centroids from a grid layer and samples multiple rasters at those locations. The next step is to join this sampled data with another functional table in your GeoPackage. VS Code Copilot can help you with this task.\n",
    "\n",
    "## Task Instructions\n",
    "\n",
    "### 1. Prepare Your Environment\n",
    "- Make sure VS Code with GitHub Copilot extension is installed and enabled\n",
    "- Have your notebook and the completed Grid Centroid Raster Sampling script open\n",
    "\n",
    "### 2. Identify the Target Table\n",
    "- First, determine which functional table in the GeoPackage you want to join with the 'green_ampt_sampled' table\n",
    "- You'll need to know:\n",
    "  - The table name\n",
    "  - The key field to join on (likely 'fid')\n",
    "  - Which fields you want to include in the final result\n",
    "\n",
    "### 3. Ask Copilot to Create the Join Script\n",
    "Here are some prompts you can use with VS Copilot:\n",
    "\n",
    "```\n",
    "\"Create a Python script that joins the green_ampt_sampled table with the [YOUR_TABLE_NAME] table in my GeoPackage. The tables should be joined on the fid field.\"\n",
    "```\n",
    "\n",
    "```\n",
    "\"I need to create a new table in my GeoPackage by joining green_ampt_sampled with [YOUR_TABLE_NAME]. Show me how to do this using SQLite and GDAL/OGR.\"\n",
    "```\n",
    "\n",
    "```\n",
    "\"Write a SQL query that joins the green_ampt_sampled table with the [YOUR_TABLE_NAME] table and explain how to execute it using Python and SQLite.\"\n",
    "```\n",
    "\n",
    "### 4. Refine Your Request\n",
    "If the initial response isn't quite what you need, try specifying more details:\n",
    "\n",
    "```\n",
    "\"The query should include the following fields from [YOUR_TABLE_NAME]: [FIELD1, FIELD2, etc.] along with all fields from green_ampt_sampled.\"\n",
    "```\n",
    "\n",
    "```\n",
    "\"I want to save the joined data as a new table called [NEW_TABLE_NAME] in the same GeoPackage.\"\n",
    "```\n",
    "\n",
    "```\n",
    "\"Can you also add code to verify that the join worked correctly by counting the number of rows and showing the first few records?\"\n",
    "```\n",
    "\n",
    "### 5. Example Implementation Template\n",
    "You can ask Copilot to fill in the details for a template like this:\n",
    "\n",
    "```python\n",
    "# Join green_ampt_sampled with another table\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths\n",
    "base_path = Path(r\"C:\\Users\\Karen\\VS Code Projects\\ASFPM-LLM-Data-Management-Workshop\")\n",
    "gpkg_path = base_path / 'Data' / 'GeoPackage' / 'selfhelp.gpkg'\n",
    "\n",
    "# Connect to the geopackage\n",
    "conn = sqlite3.connect(gpkg_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create join query\n",
    "join_query = \"\"\"\n",
    "-- Ask Copilot to create this SQL join query\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and create new table\n",
    "# Ask Copilot to complete this section\n",
    "\n",
    "# Verify the results\n",
    "# Ask Copilot to add verification code\n",
    "\n",
    "# Close connection\n",
    "conn.close()\n",
    "\n",
    "print(\"Join completed successfully!\")\n",
    "```\n",
    "\n",
    "### 6. Additional Tasks You Can Ask Copilot For\n",
    "- Creating a visualization of the joined data\n",
    "- Adding calculated fields based on the raster values\n",
    "- Exporting the results to other formats (CSV, shapefile, etc.)\n",
    "- Creating a function that automates both the sampling and joining process\n",
    "- Adding data validation to ensure the join produced correct results\n",
    "\n",
    "## Tips for Working with Copilot\n",
    "- Be specific about table and field names\n",
    "- Break complex tasks into smaller, focused requests\n",
    "- Ask Copilot to explain any code it generates that you don't understand\n",
    "- Use iterative prompts to refine the solution\n",
    "- If you're not getting what you need, try rephrasing or providing a small example\n",
    "\n",
    "Good luck with your data integration task!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flo2d_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
